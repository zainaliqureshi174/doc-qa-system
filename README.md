# ğŸ“„ Document Q&A System

A production-ready Document Q&A System built with LangChain, FAISS, and Streamlit. Upload your documents and ask questions in natural language â€” the system finds accurate, context-aware answers and shows you exactly where they came from.

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.3.25-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.45.1-red.svg)
![FAISS](https://img.shields.io/badge/FAISS-1.10.0-orange.svg)

---

## âœ¨ Features

- **Multi-Format Support** â€” Upload PDF, DOCX, and TXT files
- **Semantic Search** â€” Finds answers by meaning, not just keywords
- **Conversational Memory** â€” Ask follow-up questions naturally
- **Source Citations** â€” See exactly which part of the document answered your question
- **Multi-Document Support** â€” Upload multiple documents and query across all of them
- **Document Summarization** â€” Generate a full document summary in one click
- **Model Flexibility** â€” Switch between Groq LLM models from the UI
- **Chat History** â€” Full conversation history displayed in a clean chat interface

---

## ğŸ—ï¸ Architecture

The system works in two phases:

**Phase 1 â€” Ingestion (when a document is uploaded):**
```
Document â†’ Parse Text â†’ Split into Chunks â†’ Generate Embeddings â†’ Store in FAISS
```

**Phase 2 â€” Query (when a question is asked):**
```
Question â†’ Embed Question â†’ Search FAISS â†’ Retrieve Relevant Chunks â†’ LLM generates Answer
```

Conversational memory passes previous Q&A pairs into the LLM context so follow-up questions work naturally.

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|---|---|
| Framework | LangChain 0.3.25 |
| LLM | Groq (Llama 3.3 70B) |
| Embeddings | HuggingFace all-MiniLM-L6-v2 (local) |
| Vector Store | FAISS |
| UI | Streamlit |
| Document Loaders | PyPDF, Docx2txt, TextLoader |
| Memory | ConversationBufferWindowMemory |

---

## ğŸ“ Project Structure
```
doc_qa_system/
â”‚
â”œâ”€â”€ app.py                        # Streamlit entry point
â”œâ”€â”€ .env                          # API keys (not committed)
â”œâ”€â”€ .env.example                  # Environment variable template
â”œâ”€â”€ requirements.txt              # All dependencies
â”œâ”€â”€ README.md                     # Project documentation
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ document_processor.py     # Document loading and chunking
â”‚   â”œâ”€â”€ vector_store.py           # FAISS embeddings and retrieval
â”‚   â”œâ”€â”€ qa_chain.py               # LangChain conversational chain
â”‚   â””â”€â”€ summarizer.py             # Map-reduce summarization
â”‚
â””â”€â”€ utils/
    â””â”€â”€ helpers.py                # Configuration and utilities
```

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.9 or higher
- A Groq API key (free at [console.groq.com](https://console.groq.com))

### Installation

**1. Clone the repository:**
```bash
git clone https://github.com/zainaliqureshi174/doc-qa-system.git
cd doc-qa-system
```

**2. Create and activate virtual environment:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

**3. Install dependencies:**
```bash
pip install -r requirements.txt
```

**4. Set up environment variables:**
```bash
cp .env.example .env
```
Open `.env` and add your Groq API key:
```
GROQ_API_KEY=your_groq_api_key_here
```

**5. Run the application:**
```bash
streamlit run app.py
```

The app will open automatically at `http://localhost:8501`

---

## ğŸ’¡ How to Use

1. **Upload a document** â€” Use the sidebar to upload a PDF, DOCX, or TXT file
2. **Ask questions** â€” Type your question in the chat input
3. **View sources** â€” Click "View Sources" under any answer to see citations
4. **Follow-up questions** â€” Ask follow-up questions naturally using "he", "it", "they"
5. **Summarize** â€” Click "Generate Summary" for a full document overview
6. **Multiple documents** â€” Upload more files to query across all of them
7. **Clear chat** â€” Use "Clear Chat" to start a new conversation
8. **Reset** â€” Use "Reset All" to clear everything and start fresh

---

## ğŸ”§ Configuration

### Switching LLM Models
Select from available models in the sidebar:
- **Llama 3.3 70B** â€” Best quality, recommended
- **Llama 3.1 8B** â€” Faster responses
- **Mixtral 8x7B** â€” Alternative option

### Chunking Parameters
Adjust in `core/document_processor.py`:
```python
chunk_size=1000      # Characters per chunk
chunk_overlap=200    # Overlap between chunks
```

### Retrieval Settings
Adjust in `core/qa_chain.py`:
```python
search_kwargs={"k": 4}    # Number of chunks retrieved per query
```

---

## ğŸ“ Environment Variables

| Variable | Description | Required |
|---|---|---|
| GROQ_API_KEY | Your Groq API key | Yes |

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first.

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

*Built with LangChain Chains â€” no agents required.*
